FLUME setup and Twitter app:
---------------------------------------

step 1:
Downlaod flume package from apache:
cd /usr/local/hadoop_ecosystem/
wget https://archive.apache.org/dist/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz
tar -xvf apache-flume-1.9.0-bin.tar.gz
cd apache-flume-1.9.0-bin
rename: apache-flume-1.9.0-bin as flume-1.9.0 
----------------------------------------------------------------------------------

step 2:
vi ~/.bashhc
paste: 
export FLUME_HOME=/usr/local/hadoop_ecosystem/flume-1.9.0
export PATH=$PATH:$FLUME_HOME/bin
-----------------------------------------------------------------------------

step 3:
cd /usr/local/hadoop_ecosystem/flume-1.9.0/conf/
copy flume-conf.properties.template flume-conf.properties
copy flume-env.sh.template flume-env.sh
vi flume-env.sh
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
-----------------------------------------------------------------------------

step 4:
create a file called netcat.conf in conf directory.
cd /usr/local/hadoop_ecosystem/flume-1.9.0/conf/
gedit netcat.conf
MyAgent.sources = MySource
MyAgent.sinks = MySinks
MyAgent.channels = MyChannel
#Configure the source
MyAgent.sources.MySource.type = netcat
MyAgent.sources.MySource.bind = localhost
MyAgent.sources.MySource.port = 44444
#Configure the Channel
MyAgent.channels.MyChannel.type = memory
MyAgent.channels.MyChannel.capacity = 1000
MyAgent.channels.MyChannel.transactionCapacity = 100
#Configure the sink.
MyAgent.sinks.MySinks.type = logger
#Bind the source and sink to the channels
MyAgent.sources.MySource.channels = MyChannel
MyAgent.sinks.MySinks.channel = MyChannel
-------------------------------------------------------------------------------------------

step 5:
execute the flume agent in one terminal
cd /usr/local/hadoop_ecosystem/flume-1.9.0/
bin/flume-ng agent --conf ./conf/ -f conf/netcat.conf -Dflume.root.logger=DEBUG,console -n MyAgent
Open another terminal and run telnet
$telnet localhost 44444
Hi
Hello...
to exiit: ctrl c
---------------------------------------------------------------------------------------

step 6:
twitter connectivity:
https://github.com/kyleiwaniec/w205Project/tree/master/flume
cd /usr/local/hadoop-ecosystem/flume-1.9.0/lib
wget https://github.com/kyleiwaniec/w205Project/blob/master/flume/twitter4j-stream-4.0.4.jar
wget https://github.com/kyleiwaniec/w205Project/blob/master/flume/twitter4j-stream-2.2.6.jar
wget https://github.com/kyleiwaniec/w205Project/blob/master/flume/flume-sources-1.0-SNAPSHOT.jar
cd /usr/local/hadoop-ecosystem/flume-1.9.0/conf
add last line in flume-env.sh 
FLUME_CLASSPATH="/usr/local/hadoop_ecosystem/flume-1.9.0/lib/flume-sources-1.0-SNAPSHOT.jar"
----------------------------------------------------------------------------------

step 6:
Twitter use case: Streaming twitter app log data into hdfs.
steps for Twitter app:

https://apps.twitter.com/
create new app in the twitter -> -> create an app -> What is your primary reason for using Twitter developer tools? Student
-> Add a valid phone number
-> Activate it
-> Will use it for learning and practicing bigdata tools. I would use  Apache flume to ingest the twitter data into Hadoop-HDFS and and move it to Hive for practicing the near real time streaming data using Apache Flume. 
-> Will pull the data from twitter and analyze it using big data tools.  I would use  Apache flume to ingest the twitter data into Hadoop-HDFS and and move it to Hive for practicing the near real time streaming using Apache Flume. 
-> Yes, the app will use Tweet, Retweets and use this data for analysis in the hadoop ecosystem. I would use  Apache flume to ingest the twitter data into Hadoop-HDFS and and move it to Hive for practicing the near real time streaming using Apache Flume. 
-> I would use  Apache flume to ingest the twitter data into Hadoop-HDFS and and move it to Hive for practicing the aggregation of the near real time streaming data using Apache Flume. 
-> No, the derived information will not be available to a government entity.

Create an app.
App name: FlumeTweetsApp
Desc: This app will be used for testing Flume data ingesting using Twitter API.
Website URL : https://flumetest.com
Allow this app to signin to Twitter.
Callback URL:
https://flumetest.com

get the following values

API Key: dfsldkfjsldjf
API secret key: dashdajfhiwqflfal
Access token : dalkjflafsj
Access token secret : fdkajhfdalkh
or
----
Consumer Key (API Key): fdalfjals
Consumer Secret (API Secret): fdalkhflasfhl
Access Token:   flajflajf
Access Token Secret:  djaljflajf

---------------------------------------------------------------------------------------
step 7:
create hdfs directory
hdfs dfs -mkdir /user/twitter_data

create a twitter.conf file in /usr/local/hadoop_ecosystem/flume-1.9.0/conf
getit twitter.conf
add the following and update the above keys below:

# Naming the components on the current agent. 
TwitterAgent.sources = Twitter 
TwitterAgent.channels = MemChannel 
TwitterAgent.sinks = HDFS
# Describing/Configuring the source 
TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource
TwitterAgent.sources.Twitter.consumerKey = -----------------------------------------
TwitterAgent.sources.Twitter.consumerSecret = -------------------------------------
TwitterAgent.sources.Twitter.accessToken = ---------------------------------------
TwitterAgent.sources.Twitter.accessTokenSecret = ----------
TwitterAgent.sources.Twitter.keywords = bigdata, mapreduce, spark, sqoop, hive
# Describing/Configuring the sink 
TwitterAgent.sinks.HDFS.type = hdfs 
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://localhost:9000/user/twitter_data/
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream 
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text 
TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0 
TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000 
# Describing/Configuring the channel 
TwitterAgent.channels.MemChannel.type = memory 
TwitterAgent.channels.MemChannel.capacity = 10000 
TwitterAgent.channels.MemChannel.transactionCapacity = 1000
# Binding the source and sink to the channel 
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sinks.HDFS.channel = MemChannel
---------------------------------------------------------------------------------------------------
step 8:

Finally execute the twitter.conf file:
cd /usr/local/hadoop-ecosystem/flume-1.9.0/
bin/flume-ng agent --conf ./conf/ -f conf/twitter.conf -Dflume.root.logger=DEBUG,console -n TwitterAgent
#bin/flume-ng agent --conf ./conf/ -f conf/netcat.conf -Dflume.root.logger=DEBUG,console -n MyAgent

-----------------------------------------------------------
Output:

mohammad@mohammad:~$ hdfs dfs -ls /user/twitter_data/
20/10/13 10:15:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Found 3 items
-rw-r--r--   1 mohammad supergroup    1550592 2020-10-12 17:37 /user/twitter_data/FlumeData.1602520624122
-rw-r--r--   1 mohammad supergroup    1117599 2020-10-12 17:38 /user/twitter_data/FlumeData.1602520671298

